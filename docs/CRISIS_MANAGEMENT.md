# D-A-I-R-A Crisis Management & Incident Playbooks

**Objective:** Prepare rapid response protocols for high-impact incidents  
**Scope:** Security, operational, legal, and reputational crises  
**Severity Levels:** P0 (system down) â†’ P1 (major impact) â†’ P2 (moderate) â†’ P3 (minor)  

---

## 1. Crisis Severity Levels & Escalation

### P0: Critical System Outage

**Definition:** Platform completely unavailable OR major data loss

```
Examples:
â”œâ”€ Database corrupt/deleted
â”œâ”€ All servers offline
â”œâ”€ 50%+ user data lost
â””â”€ Payment processing failed for 24+ hours

Impact: 
â”œâ”€ Revenue loss: ~$800-1000/hour
â”œâ”€ User churn: 5-10% if >2 hours
â”œâ”€ Legal liability: Possible
â””â”€ Reputation: Severe

Response time: <15 minutes to acknowledge, <2 hours to resolve
```

**Escalation Chain:**
```
Trigger: System monitoring alert (automated)
â†“
Page on-call engineer (SMS + call)
â†“
If not resolved in 30min: Page DevOps lead
â†“
If not resolved in 1 hour: Page CTO
â†“
If not resolved in 2 hours: Page CEO + notify investors
â†“
If not resolved in 4 hours: Go public with status page post
```

### P1: Major Feature Broken

**Definition:** Core feature unusable for >10% of users

```
Examples:
â”œâ”€ Video uploads failing (100% of uploads)
â”œâ”€ Feed doesn't load for iOS users (50% of traffic)
â”œâ”€ Creator fund payouts stuck (5K creators can't cash out)
â”œâ”€ Chat messages not delivering
â””â”€ Authentication broken for specific regions

Impact:
â”œâ”€ Revenue loss: $100-500/hour
â”œâ”€ User frustration: High
â”œâ”€ Churn: 1-2% if >4 hours
â””â”€ Support tickets: 50-100/hour

Response time: <1 hour to acknowledge, <4 hours to resolve
```

### P2: Moderate Issue

**Definition:** Non-critical feature broken, impacting <10% of users

```
Examples:
â”œâ”€ Search function slow
â”œâ”€ Notifications delayed
â”œâ”€ Some users see older feed
â”œâ”€ Admin dashboard glitchy
â””â”€ Image optimization errors

Impact:
â”œâ”€ User inconvenience: Moderate
â”œâ”€ Support tickets: 10-20/hour
â””â”€ Churn: <0.5% if >24 hours

Response time: <4 hours to acknowledge, <24 hours to resolve
```

### P3: Minor Issue

**Definition:** Minor bugs or degraded performance

```
Examples:
â”œâ”€ Typo in UI
â”œâ”€ Button animation glitchy
â”œâ”€ Page loads 500ms slower
â”œâ”€ Obscure edge case bug
â””â”€ Non-critical API slow

Impact: Minimal
Response time: <24 hours
```

---

## 2. P0: Critical System Outage Playbook

### Phase 1: Detection & Acknowledgment (0-5 minutes)

**Who:** On-call engineer (automatic alert â†’ SMS + call)

**Checklist:**
```
â–¡ Acknowledge alert (click "acknowledged" in monitoring tool)
â–¡ Check all services:
  â–¡ Backend API health (/health endpoint)
  â–¡ Database connectivity (try query from bastion)
  â–¡ Redis connectivity (check cluster status)
  â–¡ Frontend CDN status (check Cloudflare dashboard)
  â–¡ Video processing (check BullMQ jobs)
â–¡ Check AWS/Heroku status pages for regional outages
â–¡ Initial diagnosis: Is this our infrastructure or cloud provider?
â–¡ Post in #emergency Slack channel: "INVESTIGATING: [Brief description]"
â–¡ Set status page to "Investigating" (status.daira.app)
```

**First Message Template:**
```
In #emergency:
"ğŸš¨ P0: [Service] down
Symptoms: [What we're seeing]
Started: [When alert fired]
Investigating: [What we're checking]
ETA: [Best guess]
"
```

### Phase 2: Rapid Triage (5-15 minutes)

**Who:** On-call engineer + CTO (if not resolved in 10min)

**Triage Matrix:**
```
Is the problem on our infrastructure?
â”œâ”€ YES: Go to "Our Infrastructure Failed" (below)
â””â”€ NO: Is it cloud provider issue?
    â”œâ”€ YES: Go to "Cloud Provider Outage" (below)
    â””â”€ NO: Go to "Data Corruption/Loss" (below)
```

**Our Infrastructure Failed**
```
Quick diagnostics:
â”œâ”€ SSH to bastion host
â”œâ”€ Check disk space: df -h (if 100% full, it's a disk space issue)
â”œâ”€ Check logs: tail -100 /var/log/app.log | grep ERROR
â”œâ”€ Check process status: ps aux | grep [app-name]
â”œâ”€ Check memory: free -h (if <500MB available, OOM killed)
â”œâ”€ Check network: ping 8.8.8.8 (network connectivity test)

If reboot solves it: REBOOT (with 30min downtime is better than 2hr investigation)
If not: Escalate to CTO
```

**Cloud Provider Outage**
```
Action:
â”œâ”€ Verify on cloud provider status page (AWS, Heroku, etc)
â”œâ”€ If confirmed outage: Prepare public statement
â”œâ”€ Message: "We're experiencing a disruption due to [provider] outage.
           Working with them to restore service. ETA: [check status page]"
â”œâ”€ Update status page every 30 minutes
â””â”€ Nothing we can do except wait (unless we have failover region)
```

**Data Corruption/Loss**
```
PANIC MODE: Do NOT touch anything
â”œâ”€ Stop application immediately (kill process)
â”œâ”€ Take database backup NOW (if still accessible)
â”œâ”€ SSH read-only to database (don't execute any writes)
â”œâ”€ Check most recent backup:
â”‚   â”œâ”€ When was it taken?
â”‚   â”œâ”€ Is it intact?
â”‚   â””â”€ How much data would we lose if we restore?
â”œâ”€ Call CEO immediately
â”œâ”€ Assess: Restore from backup (1-2hr recovery, some data loss)
          vs. Investigate corruption (4-8hr investigation, possible full data loss)
â””â”€ Decision: CEO calls the shot (restore vs investigate)
```

### Phase 3: Communication (Parallel with recovery)

**Who:** CEO (if P0 >30 minutes) or On-call engineer

**Status Page Update (Every 30 minutes during outage):**
```
Initial (5 min): 
"ğŸ”´ INVESTIGATING: Platform unresponsive. Our team is investigating the cause.
Last updated: [time]"

After 15 min:
"ğŸ”´ INVESTIGATING: We've identified [brief technical description]. 
Working to restore. ETA: ~15 minutes.
Last updated: [time]"

After 1 hour:
"ğŸŸ¡ DEGRADED: We're making progress. Expecting partial service in ~30 minutes.
Last updated: [time]"

After 2 hours:
"ğŸŸ¡ DEGRADED: We've made partial progress. Full restoration in ~30 minutes.
Users in [region] may still see delays.
Last updated: [time]"
```

**User Communication (If outage >1 hour):**
```
Post on homepage banner:
"âš ï¸ We're currently experiencing service disruptions. 
Our team is working to restore access. 
Check status.daira.app for updates."

Email to users (if email still works):
Subject: "D-A-I-R-A Service Disruption - We're Working on It"

Body:
"We experienced a service outage starting at [time]. 
Our technical team is actively working to restore full service. 
We apologize for the disruption and will update you hourly.
Latest status: [link to status page]"

Twitter/Public:
"We're aware of service disruptions affecting D-A-I-R-A users. 
Our team is investigating and working on a fix. Updates every 30 min on [status page]."
```

### Phase 4: Recovery & Monitoring (During fix)

**Who:** Engineering team + CTO

**Recovery Process:**
```
Step 1: Diagnose root cause
â”œâ”€ Check logs for ERROR/FATAL messages
â”œâ”€ Check metrics (CPU, memory, disk, network)
â”œâ”€ Correlation: When exactly did the problem start?
â”‚  â””â”€ Cross-check with deployment logs
â”‚     "Did we deploy something in the last hour?"
â””â”€ If yes: Rollback deployment immediately

Step 2: Implement fix
â”œâ”€ If infrastructure problem: Repair (restart service, free space, etc)
â”œâ”€ If application bug: Rollback or fast hotfix
â”‚  â””â”€ Fast hotfix: Quick patch (30 min), test locally, deploy
â”‚  â””â”€ Rollback: Revert to last known-good version (5 min), less risky
â”œâ”€ If external issue: Verify provider status, escalate if needed
â””â”€ Always choose speed over perfection (fix now, optimize later)

Step 3: Verify recovery
â”œâ”€ System health checks:
â”‚  â”œâ”€ Backend API responding: curl http://api/health
â”‚  â”œâ”€ Database queries working: SELECT COUNT(*) FROM users;
â”‚  â”œâ”€ Redis connected: redis-cli PING
â”‚  â””â”€ Video processing queue: bullmq jobs pending
â”œâ”€ User-facing tests:
â”‚  â”œâ”€ Load homepage (check rendering)
â”‚  â”œâ”€ Upload test video (check encoding)
â”‚  â”œâ”€ Fetch feed (check data flow)
â”‚  â””â”€ Like/comment (check writes)
â””â”€ All green? Service restored ğŸŸ¢
```

**Monitoring During Recovery:**
```
Real-time dashboard during fix:
â”œâ”€ Error rate (target: <0.1%)
â”œâ”€ API response time (target: <200ms p99)
â”œâ”€ Queue depth (BullMQ jobs backlog)
â”œâ”€ Database connections
â”œâ”€ Support ticket volume
â””â”€ User session count (should increase as recovery progresses)

Red lines that trigger rollback:
- Error rate >5%
- Response time >5s p99
- Database unavailable
- More than 50 new error tickets/min
```

### Phase 5: Post-Incident (After recovery)

**Who:** CTO + On-call engineer + CEO

**Immediate (Within 1 hour):**
```
â–¡ Update status page: "ğŸŸ¢ RESOLVED: [time]. Service restored.
    Root cause: [brief explanation]. We're continuing to monitor."

â–¡ Send user email:
Subject: "D-A-I-R-A Service Restored"

Body:
"The service disruption that started at [time] has been resolved as of [time].
Root cause: [plain English explanation, not technical jargon]

Example: "A database disk became full, preventing new data writes. 
We've freed up space and verified all data is intact."

What we're doing:
- [Action 1]: Set up alerts for disk space (prevent future incidents)
- [Action 2]: Implement backup rotation (ensure quick recovery)
- [Action 3]: Post-mortem meeting (Tuesday) to prevent recurrence

We apologize for the disruption and appreciate your patience."

â–¡ Notify investors (if Series A or later)
Subject: "Transparency: Service Incident Today"

Body:
"We experienced a [duration] service outage today starting at [time].
Root cause: [brief]
Actions taken: [brief recovery steps]
Post-mortem timeline: [when we'll have full report]
Impact: ~$[revenue loss], [user churn]%
We're implementing [safeguards] to prevent recurrence."
```

**Within 24 Hours:**
```
â–¡ Conduct incident post-mortem (1-2 hours)
  â”œâ”€ Timeline: Exact sequence of events
  â”œâ”€ Root cause: What actually broke?
  â”œâ”€ Detection lag: When did we notice vs actual start?
  â”œâ”€ Resolution time: How long to fix?
  â”œâ”€ Root causes analysis:
  â”‚  â””â”€ Was this preventable? How?
  â”‚  â””â”€ Did we miss warning signs?
  â”‚  â””â”€ Was this a known risk?
  â””â”€ Action items:
     â”œâ”€ Alert improvements (should we have caught this earlier?)
     â”œâ”€ Automation improvements (can we auto-fix in future?)
     â”œâ”€ Documentation (update runbooks for this scenario)
     â””â”€ Owner + deadline for each action

â–¡ Write incident report:
  - Share with team + investors
  - Transparency builds trust (especially after P0)

â–¡ Update status page with incident report link
```

**Within 1 Week:**
```
â–¡ Implement all action items from post-mortem
â–¡ Add monitoring for this specific failure mode
â–¡ Run incident scenario training (team practices response)
â–¡ Update runbooks with lessons learned
```

---

## 3. P0: Security Breach Playbook

### Scenario: Unauthorized Access / Data Leak

**Definition:** Attacker accessed user database, posted malware, stole credentials

### Detection (Moment breach discovered)

**Who:** On-call engineer (security alerts) OR reported by user

**Immediate actions (First 5 minutes):**
```
â–¡ Verify breach is real (check logs for evidence)
  â”œâ”€ Suspicious login: grep "failed attempts" /var/log/auth.log
  â”œâ”€ Unauthorized data export: check database audit logs
  â”œâ”€ Website defacement: check version control for unauthorized changes
  â””â”€ Example: "CREATE TABLE attacker_backdoor" in database logs
  
â–¡ If breach confirmed: ISOLATE AFFECTED SYSTEM
  â”œâ”€ Kill database connections (shut down app briefly)
  â”œâ”€ Isolate from internet (unless you can't)
  â”œâ”€ Turn off auto-backups (preserve evidence)
  â””â”€ Do NOT clean logs (might lose forensic evidence)

â–¡ Alert team immediately:
  Post in #emergency:
  "ğŸš¨ SECURITY BREACH DETECTED
  System: [Database/App/Mail]
  Time detected: [now]
  Immediate actions: [what we've done]
  Next steps: Activate security breach playbook"

â–¡ Call CEO + lead investor immediately (breach is serious)
```

### Phase 1: Containment (First hour)

**Who:** CTO + Security lead (contractor if no in-house)

**Assess damage:**
```
Questions to answer IMMEDIATELY:
1. How did attacker get in?
   â”œâ”€ SQL injection? (upgrade input validation)
   â”œâ”€ Weak password? (force password reset)
   â”œâ”€ Compromised API key? (rotate all keys)
   â”œâ”€ Cloud provider compromise? (unusual, but check)
   â””â”€ Answer: Look at access logs, git commits

2. What data was accessed?
   â”œâ”€ User emails? (PDPL breach notification required in 72h)
   â”œâ”€ Passwords? (force password reset for all users)
   â”œâ”€ Payment info? (highly regulated, serious liability)
   â”œâ”€ Creator content? (less critical, but still a breach)
   â””â”€ Answer: Check database query logs + access patterns

3. How long was attacker active?
   â”œâ”€ Minutes? (narrow blast radius)
   â”œâ”€ Hours? (moderate damage)
   â”œâ”€ Days? (SERIOUS - possible ongoing access)
   â””â”€ Answer: Correlate first unauthorized access with last

4. Is attacker STILL inside?
   â”œâ”€ Check for backdoors in code
   â”œâ”€ Check for unauthorized database users
   â”œâ”€ Check for scheduled jobs attacker created
   â””â”€ If uncertain: Assume attacker is still inside until proven otherwise
```

**Immediate containment:**
```
â–¡ Change ALL database passwords
  â”œâ”€ Rotate application DB credentials
  â”œâ”€ Rotate admin user credentials
  â”œâ”€ Rotate backup credentials
  â””â”€ Deploy new credentials to app servers (restart services)

â–¡ Rotate all API keys + secrets
  â”œâ”€ AWS access keys
  â”œâ”€ Database keys
  â”œâ”€ Payment processor keys (Fawry, etc)
  â”œâ”€ Email service keys
  â””â”€ Deploy new keys (may cause brief service disruption)

â–¡ Force password reset for all users
  â”œâ”€ Invalidate all existing sessions (users get logged out)
  â”œâ”€ Force re-login with new password on next access
  â”œâ”€ Notify users: "Security incident - please reset password"
  â””â”€ This is disruptive but necessary if passwords compromised

â–¡ Review access logs (since attack start time) for:
  â”œâ”€ Unusual data exports
  â”œâ”€ Large queries (attacker extracting data)
  â”œâ”€ Admin account access (attacker escalating privileges)
  â””â”€ Backup access (attacker might have stolen backup)

â–¡ Take snapshot of affected systems (for forensics)
  â”œâ”€ Don't run any cleanup or optimizations
  â”œâ”€ Preserve exact state for investigation
  â””â”€ Might need this for law enforcement / regulators
```

**Decision: Downtime vs. Restoration**
```
If we can contain without downtime:
â”œâ”€ Rotate credentials in background
â”œâ”€ Users don't notice
â”œâ”€ Keep service running

If we need downtime:
â”œâ”€ Go offline for 1-2 hours
â”œâ”€ Do full cleanup, verification
â”œâ”€ Communicate clearly: "Security incident requires brief maintenance"
â””â”€ Better safe than sorry (better 2hr downtime than ransomware running)
```

### Phase 2: Notification (24-72 hours)

**Who:** CEO + Legal + Communications

**PDPL Breach Notification (Required by law within 72 hours)**
```
If user data compromised:

Step 1: Report to PDPA (Egyptian authority)
â”œâ”€ Contact: PDPA office in Cairo
â”œâ”€ Provide: Description of breach, data affected, remediation steps
â”œâ”€ Timeline: Within 72 hours of discovery
â””â”€ Letter format:
   "On [date] at [time], we discovered unauthorized access to 
    D-A-I-R-A servers affecting [X] user records.
    
    Data compromised: [user emails, phone numbers, etc - be specific]
    Number of users affected: [X users]
    Likely cause: [SQL injection / weak password / etc]
    When discovered: [date/time]
    Corrective actions taken: [all mitigations above]
    
    We are notifying affected users and have implemented [safeguards]
    to prevent recurrence."

Step 2: Notify affected users
â”œâ”€ Method: Email + SMS (so they see it)
â”œâ”€ Timing: Within 72 hours of discovery
â”œâ”€ Message template:

Subject: "Important: Security Incident Notification (Action Required)"

Body:
"We're writing to inform you of a security incident that may have affected 
your D-A-I-R-A account.

What happened:
On [date], we discovered unauthorized access to our servers. An attacker 
accessed user data including [list specifically: email, phone, birth date, etc].

What was NOT compromised:
[List what wasn't touched, to reassure]

What we're doing:
1. We've secured the system and rotated all security credentials
2. We've notified law enforcement and are cooperating with investigation
3. Your password has been invalidated - please reset it on next login
4. We recommend changing password on any other sites where you used 
   the same password

What you should do:
1. Change your D-A-I-R-A password immediately (forced on next login)
2. Change passwords on other sites (if you reused password)
3. Monitor your email/phone for suspicious activity
4. Consider credit freeze (if payment info was compromised)

We're providing 12 months free credit monitoring service [link]

We sincerely apologize for this incident and appreciate your patience
as we work to ensure this doesn't happen again."

Step 3: Prepare for media questions
â”œâ”€ Press release: "D-A-I-R-A Addresses Security Incident" (see below)
â”œâ”€ Contact 2-3 tech journalists with your version first (better than hearing from hackers)
â””â”€ Be honest + transparent (lying about breach makes it worse)
```

### Phase 3: Investigation & Forensics

**Who:** External security firm + internal CTO

**Hire incident response firm** (if you don't have in-house expertise)
```
Cost: $5K-20K for full forensics
Timeline: 1-2 weeks for full report
What they do:
â”œâ”€ Detailed timeline of attacker activity
â”œâ”€ Root cause analysis
â”œâ”€ Identification of attacker (if possible)
â”œâ”€ Recommendations for prevention
â””â”€ Report suitable for law enforcement / regulators
```

**Things to preserve for forensics:**
```
âˆš All database query logs (binary format, don't compress)
âˆš All application logs (ERROR/FATAL messages, access patterns)
âˆš All firewall logs (what was accessed from outside)
âˆš Disk snapshots (raw copy of affected systems)
âˆš Git commit history (check for unauthorized code changes)
âˆš Email/messaging history (Slack, email during breach time)
âˆš Monitoring alerts (when did systems first act weird)

Do NOT:
âœ— Overwrite logs during investigation
âœ— Run cleanup scripts (destroys evidence)
âœ— Allow attacker to continue (assume they're still inside)
```

---

## 4. P0: Government Blocking/Threats Playbook

### Scenario: NTRA threatens platform shutdown OR blocks access

### Detection & Immediate Response (Hour 0-1)

**Who:** CEO + Legal + Government liaison (fixer)

**Verification:**
```
â–¡ Is this a legitimate government request?
  â”œâ”€ Check if letter is on official government letterhead
  â”œâ”€ Verify sender email domain is real government domain
  â”œâ”€ Call phone number on letter (verify it's real)
  â””â”€ If phishing: Ignore + report to cybersecurity team

â–¡ If legitimate: What exactly are they asking?
  â”œâ”€ Remove specific content? (identify which posts/accounts)
  â”œâ”€ Shut down entire platform? (temporary vs permanent)
  â”œâ”€ Provide user data? (PII access request)
  â”œâ”€ Comply with content policy changes? (new filtering rules)
  â””â”€ Save exact wording for legal team
```

**Initial Communication:**
```
Call government liaison/fixer immediately:
"We received [type of] request from [agency]. 
Key ask: [what they want]
Deadline: [when they want response]
We're reviewing with legal team. What's your read on this?
Can you make exploratory call to understand their concerns?"

Fixer's job: Find out if this is serious or negotiable
```

### Phase 1: Legal Review (Hour 1-4)

**Who:** Egyptian lawyer + International counsel

**Legal assessment:**
```
Questions for Egyptian lawyer:
1. Is this request legal under Egyptian law?
   â”œâ”€ Yes, we must comply (or face consequences)
   â”œâ”€ Maybe, it's grey area (we can negotiate)
   â””â”€ No, we can refuse (risky but defensible)

2. What's the legal mechanism?
   â”œâ”€ Cybercrime Law article 25 (false news)? â†’ Most common
   â”œâ”€ Telecom law? â†’ Usually regarding spectrum/licensing
   â”œâ”€ Morality law? â†’ Vague, dangerous
   â””â”€ Understanding mechanism helps predict likelihood of enforcement

3. What happens if we don't comply?
   â”œâ”€ NTRA blocks ISPs from serving us? (effective ban)
   â”œâ”€ Fines? (amounts range wildly)
   â”œâ”€ Criminal charges against CEO? (depends on severity)
   â””â”€ Mix of all three? (most likely)

4. Can we negotiate scope?
   â”œâ”€ "Instead of removing all political content, 
       we'll add disclaimer labels?"
   â”œâ”€ "We'll remove the specific post, not entire account?"
   â”œâ”€ "We need legal documentation to comply properly - 
       can you provide written guidance?"
   â””â”€ Negotiation buys time + may soften request
```

**International counsel input:**
```
Questions for international lawyer:
1. Can we operate from outside Egypt if blocked?
   â”œâ”€ VPN/proxy to circumvent block? (risky, shows defiance)
   â”œâ”€ Move servers to other countries? (doesn't help Egypt users)
   â”œâ”€ Partner with international company? (can help, but slow)
   â””â”€ Likely answer: Limited options if blocked

2. What's our legal exposure?
   â”œâ”€ Personal liability for CEO? (depends on laws)
   â”œâ”€ Corporate liability? (liability limited if LLC structured right)
   â”œâ”€ Tax/fund liability? (freezing of accounts, assets)
   â””â”€ Insurance coverage? (hopefully yes, but may have exclusions)

3. Should we fight this publicly?
   â”œâ”€ Pro: "D-A-I-R-A stands for free speech" (good PR)
   â”œâ”€ Con: Antagonizes government, speeds blocking, no legal upside
   â””â”€ Recommendation: Fight privately, communicate neutrally
```

### Phase 2: Response Options (Hour 4-24)

**Who:** CEO + Government liaison + Legal team

**Option A: Full Compliance (Safest)**
```
Action: Do exactly what government asks
â”œâ”€ Remove content immediately
â”œâ”€ Provide user data (if requested)
â”œâ”€ Implement content filters they specify
â””â”€ Promise ongoing cooperation

Pros:
âœ… Avoid blocking
âœ… Avoid fines
âœ… Avoid criminal charges
âœ… Appear cooperative (helps with future requests)

Cons:
âŒ Sets precedent for more requests
âŒ May need to remove legitimate content
âŒ Damages free speech reputation
âŒ Eventually they ask for everything (slippery slope)

Timeline to comply: 24-48 hours (what they usually demand)
```

**Option B: Partial Compliance + Negotiation (Medium)**
```
Action: Comply with parts you can defend, negotiate others

Example letter to government:

"Thank you for your request regarding [content].

D-A-I-R-A is committed to complying with Egyptian law. 
We've reviewed your request carefully and want to ensure we respond 
appropriately.

Regarding [specific content]:
We can [action 1] while we need clarification on [request 2].

To ensure we're in full compliance, we'd appreciate:
1. Written clarification on which specific content violates which law
2. Definition of "false information" vs. "opinion" for this context
3. Timeline and process for appeals if we believe request is overly broad

We take regulatory compliance seriously and want to work closely 
with NTRA to ensure we're meeting all obligations while continuing 
to serve Egyptian users."

Pros:
âœ… Buy time (government has to explain)
âœ… Create paper trail (documents for court, if it goes there)
âœ… Negotiate scope (maybe they'll accept some filtering)
âœ… Appear cooperative but not spineless

Cons:
âŒ May anger government
âŒ Could speed up blocking
âŒ Requires strong legal defense to back up "no" on parts you refuse

Timeline: Government usually responds in 5-10 days
```

**Option C: Refuse (Nuclear - Only if illegal request)**
```
Example response:

"Thank you for your request. After careful legal review with our 
Egyptian and international counsel, we believe this request exceeds 
the scope of Egyptian law and would require us to violate principles 
of due process and user rights.

We are committed to complying with legal, specific takedown requests 
that identify particular content violating specific laws. We believe 
this request is overly broad and lacks the specificity required.

We respectfully decline this request but remain open to dialogue 
about how we can work with NTRA constructively."

Pros:
âœ… Stand for principles
âœ… Set legal precedent (if case goes to court)
âœ… International support (press, human rights groups)
âœ… Win reputation as platform defending free speech

Cons:
ğŸ”´ Platform likely gets blocked within days
ğŸ”´ Possible fines ($50K-500K)
ğŸ”´ Criminal charges possible
ğŸ”´ CEO may be detained for questioning
ğŸ”´ Business dies unless you can operate unblocked

Timeline to blocking: 2-7 days

ONLY use this option if:
- Request is clearly illegal (not grey area)
- You have investor/funding support
- You're willing to sacrifice Egypt market to make statement
- You have international PR support ready
```

**My Recommendation:**
```
Most likely scenario: Option B (partial compliance + negotiation)
â”œâ”€ Appear cooperative (avoid knee-jerk blocking)
â”œâ”€ Buy time to understand their real concerns
â”œâ”€ Negotiate on defensible grounds
â”œâ”€ Implement reasonable content policy changes
â””â”€ Continue operating

Avoid Option C unless absolutely forced - it's existential risk
```

### Phase 3: Public Communication (Parallel)

**If government request becomes public:**

**What NOT to do:**
```
âŒ Ignore it (shows you're not complying)
âŒ Lie about it (coverup is worse than incident)
âŒ Panic messaging (makes you look weak)
âŒ Attack government (makes things worse)
```

**What to do:**

**Public Statement Template:**
```
"D-A-I-R-A received a content request from [agency] and is reviewing 
it carefully with our Egyptian legal team. 

We are committed to:
- Complying with Egyptian law
- Protecting user privacy
- Maintaining trust with our community
- Working constructively with regulators

We take regulatory requests seriously and will respond appropriately 
within the legal timeframe. We appreciate our users' patience as we 
work through this process."

Key principle: "cooperative + professional + no details"
```

**What to tell investors (Private):**
```
Email subject: "Transparency: Regulatory Request"

"We received a content request from [agency] asking us to [action]. 
We're reviewing this with our legal team in Egypt. 

Our assessment:
- This is a [legal/grey/borderline] request under Egyptian law
- Our response: [Option A/B/C above]
- Timeline: We expect resolution within [X days]
- Risk: [Low/Medium/High] - [reasoning]

We'll update you as the situation develops. This is fairly common 
for platforms operating in Egypt and we're handling it appropriately."
```

### Phase 4: Execution

**If you go with Option B (recommended):**
```
â–¡ Send response letter to government within legal timeframe (usually 48-72h)
â–¡ Implement content removal that's defensible
  â”œâ”€ Remove content that violates law (easy cases)
  â”œâ”€ Add labels to borderline content (not removal)
  â””â”€ Keep content you can defend (refuse unjustifiable requests)

â–¡ Monitor for government response
  â”œâ”€ If they're satisfied: You've bought time âœ…
  â”œâ”€ If they escalate: Prepare for blocking scenario

â–¡ If blocking happens: Activate Crisis Response (below)
```

---

## 5. Crisis Response: What To Do If Blocked

### Scenario: NTRA has blocked all ISPs from serving D-A-I-R-A

### Phase 1: Immediate Actions (Hour 0-4)

**Confirm the block:**
```
â–¡ Test from multiple providers in Egypt:
  â”œâ”€ Vodafone
  â”œâ”€ Orange
  â”œâ”€ Etisalat
  â”œâ”€ All blocked â†’ Confirmed NTRA action

â–¡ Check ISP response codes:
  â”œâ”€ DNS failure? (ISPs blocking at DNS level)
  â”œâ”€ TCP RST? (ISPs blocking at network level)
  â”œâ”€ HTTP 403? (ISPs blocking at content filter)
  â””â”€ Helps determine if we can circumvent with DNS/proxy solutions

â–¡ Notify users:
  Post on homepage (for international users):
  "D-A-I-R-A service has been restricted in Egypt. Users in Egypt
  can access via VPN. We're working with regulators to restore access."
  
â–¡ Update status page: "Platform Restricted in Egypt"

â–¡ Email team:
  Subject: "Platform blocked in Egypt - Crisis mode activated"
```

### Phase 2: Circumvention Planning (Hour 4-24)

**Options to consider:**

**Option 1: Mirror Sites / Alternative Domains**
```
Setup alternative DNS:
â”œâ”€ Buy new domain (daira-app.com, daira-tv.net, etc)
â”œâ”€ Point to same backend
â”œâ”€ Post instructions: "If blocked, try [alternative domain]"
â””â”€ Limitation: NTRA can block these too within days

Pro: Gets users back quickly
Con: Cat-and-mouse game (NTRA keeps blocking, we keep redirecting)
Viability: Works for 1-2 weeks maximum
```

**Option 2: VPN / Proxy Recommendations**
```
Post on public channels:
"Access D-A-I-R-A via VPN:
- ExpressVPN (works in Egypt)
- NordVPN
- Surfshark"

Pro: Users can still access
Con: Terrible user experience, many users won't bother
Viability: Loses 70-80% of users
```

**Option 3: Mobile App Only**
```
Distribute iOS/Android apps via:
â”œâ”€ Direct links (APK for Android)
â”œâ”€ Private TestFlight (iOS)
â””â”€ Avoid app stores (can be blocked too)

Pro: Mobile apps may not get filtered
Con: Many users have Android, older phones
Viability: Partial access for tech-savvy users
```

**Option 4: Exit Egypt + Focus on Other Markets**
```
Pause Egypt operations:
â”œâ”€ Transfer user data to safe location
â”œâ”€ Apologize to Egypt users
â”œâ”€ Redirect to MENA operations (Saudi, UAE)
â”œâ”€ Wait for political situation to change (months/years)
â””â”€ Re-enter if blocked is lifted

Pro: Protects company, redirects to profitable markets
Con: Gives up Egypt market (your original target)
Viability: Fallback option if blocking is indefinite
```

### Phase 3: Stakeholder Communication

**Tell investors immediately:**
```
Subject: CRITICAL - Platform Blocked in Egypt

"At [time] today, D-A-I-R-A was blocked by NTRA (Egyptian telecom regulator)
at the ISP level.

This blocks all Egyptian users from accessing the platform. 
International users are unaffected.

Timeline:
- We received government request [X days ago]
- We responded [with Option B negotiation]
- Government escalated to ISP-level blocking instead of continued negotiation

Next steps:
1. We're attempting negotiation with NTRA via our government liaison
2. We're evaluating operation in other MENA countries
3. We'll determine within 48 hours if this is temporary or permanent

Financial impact:
- 70% of our user base is in Egypt
- Revenue loss: ~$2K/day
- Cash runway: [X days] before we need capital

Risk assessment: This blocks the Egypt market. We're evaluating pivoting to 
Saudi Arabia / UAE or returning to Egypt once situation changes."
```

**Tell employees:**
```
All-hands meeting:
"As many of you know, D-A-I-R-A has been blocked in Egypt today. 
This is a significant setback. Here's what we know:

What happened:
[Explain government interaction]

Why it happened:
[Explain which content/policies government objected to]

What it means:
- Egypt market is paused (unknown duration)
- Revenue in Egypt drops to ~$0
- We're re-evaluating strategy

Options we're considering:
A. Negotiate with government to unblock
B. Operate in other MENA markets (Saudi, UAE)
C. Shut down and preserve capital

My commitment:
- I will keep you updated daily
- We're not abandoning the team
- Everyone's jobs are secure [for now] while we figure this out
- We're exploring all options

Questions? Let's discuss."

Morale: This is devastating. Be honest, don't pretend to have answers you don't.
```

---

## 6. Crisis Communication Templates

### Press Release: Security Breach

```
FOR IMMEDIATE RELEASE

D-A-I-R-A Addresses Security Incident; Implementing Enhanced Protections

Cairo, Egypt â€“ D-A-I-R-A, a social video platform for creators, 
today announced it discovered and addressed a security incident 
that may have affected user data.

Details:
- Date: [Date]
- Data affected: [specific: emails, phone numbers - do NOT exaggerate]
- Users impacted: [X users]
- Unauthorized access: [period of access, if known]

Response:
- Immediate: Secured systems, rotated credentials, invalidated user sessions
- 24-48 hours: Notified affected users and authorities (PDPA)
- Ongoing: Implemented enhanced security measures

Actions for users:
- All users must reset passwords (required on next login)
- Change passwords on other sites if you reused this password
- Consider credit monitoring (1 year free provided)
- Monitor email/phone for suspicious activity

D-A-I-R-A is committed to protecting user data and regrets this incident.

For more information, contact: security@daira.app
```

### Press Release: Service Outage

```
FOR IMMEDIATE RELEASE

D-A-I-R-A Service Restored After Planned Maintenance

Cairo, Egypt â€“ D-A-I-R-A restored full platform functionality at 
[time] on [date] after a [duration] service disruption.

What happened:
The platform was temporarily unavailable due to [brief explanation].

Root cause:
[Technical explanation in plain language]

Why it took so long:
[Brief explanation of complexity / why we couldn't fix faster]

What we're doing:
1. [Action 1 to prevent recurrence]
2. [Action 2]
3. [Action 3]

We apologize for the disruption and appreciate user patience.

For updates, visit: status.daira.app
```

### Email: Service Disruption Apology

```
Subject: Our Apologies For Yesterday's Service Disruption

Hi [User],

Yesterday, D-A-I-R-A experienced a service disruption lasting [duration], 
and we're deeply sorry.

What we learned:
[Technical issue + why we missed it + how we'll prevent it]

How we're fixing it:
[List 2-3 concrete actions]

Thank you for sticking with us through this. We know how important 
it is for you to have a reliable platform, and we're committed to 
doing better.

If you experienced any issues, please email support: support@daira.app

Best regards,
The D-A-I-R-A Team
```

---

## 7. Crisis Management Roles & Responsibilities

### War Room Team (Activated for P0 incidents)

```
Role: CEO
â”œâ”€ Decision maker
â”œâ”€ External communications (investors, press)
â”œâ”€ Final authority on major decisions
â””â”€ Works 18-20 hours during major incident

Role: CTO / Head of Engineering
â”œâ”€ Technical lead
â”œâ”€ Direct incident response (or delegates to on-call)
â”œâ”€ Provides technical updates to CEO
â””â”€ Calls all-hands if major incident

Role: Head of Legal
â”œâ”€ Compliance obligations (PDPL, government requests)
â”œâ”€ Statement review (before release)
â”œâ”€ External counsel coordination
â””â”€ Contracts/insurance claims

Role: Head of Communications / PR
â”œâ”€ Status page updates (every 30 min during major incident)
â”œâ”€ User communications (emails, social media)
â”œâ”€ Press release drafting
â””â”€ Media response

Role: Head of Support
â”œâ”€ Tracking support tickets
â”œâ”€ Responding to user concerns
â”œâ”€ Escalating urgent issues
â””â”€ User sentiment monitoring

Role: On-Call Engineer (During incident)
â”œâ”€ Investigates root cause
â”œâ”€ Implements fix
â”œâ”€ Technical liaison to CTO
â””â”€ Works until resolved
```

### Escalation Chain

```
Trigger: P0 detected (automated alert)
â†“
Page on-call engineer (SMS + call)
â†“
On-call diagnoses for 15 minutes
â†“
If not fixed in 15 min: Page CTO
â†“
If not fixed in 30 min: Page CEO + convene war room
â†“
If not fixed in 1 hour: Contact external incident response firm
â†“
If not fixed in 2 hours: Notify investors (all Series A+)
â†“
If not fixed in 4 hours: Public statement required
```

---

## 8. Monthly Incident Drills

**Every month, run one scenario:**

```
Month 1: Database failure
â”œâ”€ Kill database, see how fast we recover
â”œâ”€ Test backup restore procedure
â””â”€ Measure: How many minutes to restore?

Month 2: Security breach
â”œâ”€ Simulate attacker access to prod
â”œâ”€ Run through notification procedures
â””â”€ Measure: How many hours to notify users?

Month 3: Government request
â”œâ”€ Simulate NTRA takedown demand
â”œâ”€ Practice negotiation response
â””â”€ Measure: Response time, decision quality

Month 4: Major feature broken
â”œâ”€ Break payments (simulate)
â”œâ”€ Practice troubleshooting
â””â”€ Measure: Detection lag + fix time

Month 5-12: Repeat
```

**Scoring:**
```
Grade A: Detected <30min, resolved <2 hours
Grade B: Detected 30min-1 hour, resolved 2-4 hours
Grade C: Detected 1-2 hours, resolved >4 hours
Grade F: Not detected / poor response

Target: All incidents Grade B or better
Escalation: Grade C+ triggers post-mortem
```

---

**Owner:** CTO + CEO  
**Review Frequency:** Quarterly  
**Last Updated:** [Today]  
**Next Drill:** [Date of next incident simulation]  

**Remember:** You'll have a crisis. The question is whether you're ready.
